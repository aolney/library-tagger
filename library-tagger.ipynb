{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "ifsharp"
   },
   "source": [
    "# Library Tagger\n",
    "\n",
    "# IN PROGRESS NOT FUNCTIONAL\n",
    "\n",
    "This notebook is intended to function as a one-time script for rationalizing a collection of PDFs scattered across a filesystem.\n",
    "\n",
    "### Motivation\n",
    "- Every project has a collection of PDFs.\n",
    "- Closely related projects duplicate PDFs, wasting space.\n",
    "- Even carefully curated PDFs in this context are either:\n",
    "    - Dropped into a giant folder organized by author/year\n",
    "    - Split across subfolders denoting topic (e.g. `logic`) but topics are task-specific and debatable\n",
    "\n",
    "The purpose of this project is to collect PDFs across a filesystem and perform the following operations:\n",
    "\n",
    "1. Rename the PDF by\n",
    "\n",
    "    a. Extracting Author/Title/Year using [GROBID](https://github.com/kermitt2/grobid)\n",
    "    \n",
    "    b. Renaming the file [in TagSpaces style](https://docs.tagspaces.org/tagging#file-tagging-based-on-filename) with\n",
    "    \n",
    "    - FirstAuthorLastName~Year Title1 Title2 Title3 (where these are non stop words) followed by\n",
    "    \n",
    "    - [Tag1 ... TagN].pdf where TagX is\n",
    "    \n",
    "      - Each directory of the PDF's current file path\n",
    "      - Author lastnames of the PDF\n",
    "      - Keywords of the PDF\n",
    "      - Year of the PDF\n",
    "      - Title words of the PDF (stopwords removed)\n",
    "      - Eventually it would be good for this to include all my publications that cite this paper\n",
    "\n",
    "2. Move the PDF to a common folder, e.g. `/z/aolney/library`\n",
    "\n",
    "3. Deposit a symbolic link in the present directory with the old name linking to the new file\n",
    "\n",
    "At this point only a subset of [TagSpaces features](https://docs.tagspaces.org/userinterface.htm) will be needed to organize/search PDFs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Get PDFs\n",
    "\n",
    "Set one or more root directories and recursively find all PDFs. \n",
    "Save this information as a flat file and then copy all PDFs to a large working directory for GROBID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "ifsharp"
   },
   "outputs": [],
   "source": [
    "//The root directories of all PDFs\n",
    "let rootDirectories = [ \"/home/aolney/Downloads\" ]\n",
    "\n",
    "//The working folders for GROBID\n",
    "let grobidPdfFolder = \"/y/pdf/\"\n",
    "let grobidOutputFolder = \"/y/grobid/\"\n",
    "\n",
    "open System.IO\n",
    "let rec GetAllFiles (extension:string) dirs =\n",
    "    if Seq.isEmpty dirs then Seq.empty else\n",
    "        seq { yield! dirs |> Seq.collect Directory.EnumerateFiles\n",
    "              yield! dirs |> Seq.collect Directory.EnumerateDirectories |> GetAllFiles extension } |> Seq.filter( fun filePath -> filePath.ToUpper().EndsWith( extension ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "ifsharp"
   },
   "outputs": [],
   "source": [
    "let allPdfs = rootDirectories |> GetAllFiles \"PDF\"\n",
    "\n",
    "File.WriteAllLines(\"pdfList.txt\", allPdfs )\n",
    "\n",
    "Directory.CreateDirectory( grobidPdfFolder )\n",
    "Directory.CreateDirectory( grobidOutputFolder )\n",
    "\n",
    "allPdfs\n",
    "|> Seq.iter( fun filePath ->\n",
    "    let newName = filePath.Replace(Path.DirectorySeparatorChar.ToString(), \"@\")\n",
    "    let newPath = Path.Combine( grobidFolder, newName )\n",
    "    if File.Exists(newPath) |> not then File.Copy( filePath, newPath )\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## PDF to XML\n",
    "\n",
    "Using [GROBID command line](https://grobid.readthedocs.io/en/latest/Grobid-batch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "function doGrobid() {\n",
    "    java -Xmx4G -jar /z/aolney/repos/grobid-0.5.1/grobid-core/build/libs/grobid-core-0.5.1-onejar.jar -gH /z/aolney/repos/grobid-0.5.1/grobid-home -dIn $1 -dOut $2 -exe processHeader\n",
    "    #processFullText\n",
    "}\n",
    "\n",
    "doGrobid /y/pdf/ /y/grobid > grobid.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Grobid XML to tags\n",
    "\n",
    "- Author lastnames of the PDF\n",
    "- Keywords of the PDF\n",
    "- Year of the PDF\n",
    "- Title words of the PDF (stopwords removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "ifsharp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[|\"/y/grobid/@home@aolney@Downloads@0013189x17725519.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@0034654317726529.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@033.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@0402.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1-s2.0-S000437020300122X-main.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1-s2.0-S0163638311000713-main.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1-s2.0-S0749596X17300013-main.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1-s2.0-S0895435617313069-main.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1-s2.0-S1364661317301316-main.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1-s2.0-S1364661319300610-main.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@10.1.1.467.9994.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@10.1.1.720.6456.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1308.5499.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@13_Mayer.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1465.full.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1506.05908.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1510.04342.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1512.00765.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@158-Article Text-1551-1-10-20180630.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1590-1587-1-PB.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@16-0955.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1602.06979.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1604.00289.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1607.00570.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1611.09268v2.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1704.06498.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@1985TrabassoJMemLang.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2-03-00-00-7.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2011-Yudelson-3885.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2015-maas.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2016-05-26_ho-6.glmpath1.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2016-1003_ICA_Workshop_Final_Report_2016.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2017_84305D.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@20182507.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2018_84305A.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@20468414.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2289064.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@239-Article Text-1310-1-10-20170919.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2484-4316-1-PB.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2521a19806162fe68d58ada392aeff1f1b93.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@2685797.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@271-Article Text-1369-1-2-20171215.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@275-Article Text-1378-4-2-20180125.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@27699697.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@293-Article Text-1468-1-4-20180228.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@293217cf8b24735ba2e53a65cb2b0fabd7d2.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@314-Other-1519-1-15-20180608.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@318-Data Analysis-1501-1-4-20180515.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@318-Other-1504-1-4-20180516.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@319-Other-1669-1-15-20181106.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@319-Other-1671-1-15-20181106.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@333-Article Text-1516-1-2-20180606.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@334-Article Text-1552-1-2-20180630.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@334-Article Text-1556-1-10-20180630 (1).tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@334-Article Text-1556-1-10-20180630.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@335-Article Text-1557-2-2-20180705.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@336-Article Text-1560-1-2-20180710.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@337-Article Text-1561-1-2-20180710.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@338-Article Text-1562-1-2-20180710.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@341-Article Text-1566-1-2-20180710.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@343-Article Text-1569-1-2-20180710.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@345-Article Text-1571-1-2-20180711.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@347-Article Text-1575-1-2-20180711.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@348-Article Text-1576-2-2-20180711.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@349-Article Text-1577-1-2-20180711.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@35089a6.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@351-Article Text-1580-1-2-20180711.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@352-Article Text-1579-1-2-20180711.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@383-Article Text-1610-1-2-20180802.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@387-Article Text-1620-1-2-20180818.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@394-Article Text-1666-1-2-20181105.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@40031454.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@405-Article Text-1771-1-18-20190417.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@426-Article Text-1785-1-4-20190510.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@427-Article Text-1789-1-2-20190514.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@4273f0d3d4acdac20ee49c76ba3aa3afe7a5.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@42888205.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@56cb26c18092a96153de37523da64c8b7dbf.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@5f5cfc7e1df9105459a68e96f82fbb047f52.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@69-1800.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@7cd6baf72538ffb51fce4e850e7d8a5a4c53.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@80d83859381c40f2488a1c390eec732ebea5.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@82-Article Text-866-1-10-20151027.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@84_EDM-2014-Full.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@87611.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@8998395225 Andrew Test Olney Test - Full Application PDF Jun 27, 2018 at 7_19 PM.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@9783642311864-c2.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@AAAI00-084.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@AEIKHFC.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@A_1025708916924.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@Activity2B-Models.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@Agrawal_Gans_Goldfarb  Prediction.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@AlevenEtAl-ExampleTracingTutors-IJAIED2009.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@Alexander2017.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@Amazon.com - Online Return Center.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@Arora2017.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@AutoIDCards.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@BBS1999.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@BDHKGEA.tei.xml\";\n",
       "  \"/y/grobid/@home@aolney@Downloads@BaayenMilin2010.tei.xml\"; ...|]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let allXML = [ grobidOutputFolder ] |> GetAllFiles \"XML\"  \n",
    "allXML |> Seq.toArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "ifsharp"
   },
   "outputs": [],
   "source": [
    "type ReviewerInfo =\n",
    "    {\n",
    "        Name: string\n",
    "        Order: int\n",
    "        Hash : int\n",
    "        //Some of these are really only useful for debugging\n",
    "        Title : string\n",
    "        Text : string\n",
    "        File : string\n",
    "    }\n",
    "    \n",
    "let GetName first last =\n",
    "    match first,last with\n",
    "    | Some(f), Some(l) -> f + \" \" + l\n",
    "    | None, Some(l) -> l\n",
    "    | Some(f), None -> f\n",
    "    | None, None -> \"\"\n",
    "    \n",
    "//let xml = files |> Seq.head |> System.IO.File.ReadAllText\n",
    "\n",
    "let spaceRegex = new System.Text.RegularExpressions.Regex(@\"\\s+\");\n",
    "let NormalizeText ( text : string ) =\n",
    "    spaceRegex.Replace( text, \" \" ).Trim().ToLower()\n",
    "\n",
    "let ExtractInfo xmlFile = \n",
    "    let doc = new System.Xml.XmlDocument();\n",
    "    let xml = xmlFile |> System.IO.File.ReadAllText\n",
    "    doc.LoadXml(xml);\n",
    "\n",
    "    let nsmgr = new System.Xml.XmlNamespaceManager(doc.NameTable)\n",
    "    nsmgr.AddNamespace(\"tei\",  \"http://www.tei-c.org/ns/1.0\")\n",
    "\n",
    "    let theTitle  = doc.SelectSingleNode(@\"//tei:title\", nsmgr).InnerText\n",
    "    let theAbstract = doc.SelectSingleNode(@\"//tei:abstract\", nsmgr).InnerText\n",
    "    let theText = \n",
    "        doc.SelectNodes(@\"//tei:text//tei:p\", nsmgr) \n",
    "        |> Seq.cast< System.Xml.XmlNode> \n",
    "        |> Seq.map( fun x -> \n",
    "                   let directChildren = x.ChildNodes |> Seq.cast< System.Xml.XmlNode> |> Seq.map (fun x -> x.Value) //unlike InnerText, ignores child descendent nodes\n",
    "                   String.concat \" \" directChildren\n",
    "                  )\n",
    "        |> String.concat \" \"\n",
    "        |> NormalizeText\n",
    "\n",
    "    let authors = doc.SelectNodes(@\"//tei:sourceDesc//tei:persName\", nsmgr)\n",
    "    let reviewerInfos =\n",
    "        authors \n",
    "        |> Seq.cast< System.Xml.XmlNode> \n",
    "        |> Seq.mapi( fun i x -> \n",
    "                   let forename = \n",
    "                       match x.[\"forename\"] with\n",
    "                       | null -> None\n",
    "                       | f -> Some(f.InnerText)\n",
    "                   let surname = \n",
    "                       match x.[\"surname\"] with\n",
    "                       | null -> None\n",
    "                       | s -> Some(s.InnerText)\n",
    "                    //originally we hashed on the text, but the file name has the full path, so that is probably better\n",
    "                   //{Name = (GetName forename surname); File = xmlFile; Text = theText; Order = i; Title = theTitle; Hash = (hash theText) }\n",
    "                   {Name = (GetName forename surname); File = xmlFile; Text = theText; Order = i; Title = theTitle; Hash = (hash xmlFile) }\n",
    "                  )\n",
    "    //\n",
    "    if reviewerInfos |> Seq.isEmpty then\n",
    "        None\n",
    "    else\n",
    "        Some( reviewerInfos )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "ifsharp"
   },
   "outputs": [],
   "source": [
    "let reviewerInfosWithHash =\n",
    "    files\n",
    "    |> Seq.choose ExtractInfo \n",
    "    |> Seq.collect id\n",
    "    //|> Array.ofSeq\n",
    "\n",
    "//for compression purposes, map hash to small integer\n",
    "let hashHash =\n",
    "    reviewerInfosWithHash\n",
    "    |> Seq.map( fun ri -> ri.Hash )\n",
    "    |> Seq.distinct\n",
    "    |> Seq.mapi( fun i h -> h,i)\n",
    "    |> Map.ofSeq\n",
    "\n",
    "let reviewerInfos = \n",
    "    reviewerInfosWithHash\n",
    "    |> Seq.map( fun ri -> { ri with Hash=hashHash.[ri.Hash]})\n",
    "    \n",
    "let authorOutput = \n",
    "    reviewerInfos\n",
    "    |> Seq.map( fun ri ->  ri.Hash.ToString() + \"\\t\" + ri.Name + \"\\t\" + ri.Order.ToString()  )\n",
    "\n",
    "//the data layout is expected by pke\n",
    "let textOutput =\n",
    "    reviewerInfos\n",
    "    |> Seq.distinctBy( fun ri -> ri.Hash )\n",
    "    |> Seq.map( fun ri -> ri.Hash.ToString() + \"\\t\" + ri.Text + \"\\t\" + ri.Title + \"\\t\" + ri.File)\n",
    "    \n",
    "System.IO.File.WriteAllLines( \"authors.tsv\", authorOutput )\n",
    "System.IO.File.WriteAllLines( \"texts.tsv\", textOutput )\n",
    "System.IO.Directory.CreateDirectory( \"texts\") |> ignore\n",
    "for ri in reviewerInfos |> Seq.distinctBy( fun ri -> ri.Hash ) do\n",
    "    let outputPath = System.IO.Path.Combine( \"texts\", ri.Hash.ToString() + \".txt\" )\n",
    "    System.IO.File.WriteAllText( outputPath, ri.Text )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "ifsharp"
   },
   "source": [
    "## Get Keyphrases\n",
    "\n",
    "We use [PKE KP-Miner](https://boudinfl.github.io/pke/build/html/unsupervised.html#kpminer) b/c it has [the best unsupervised performance on SemEval2010](http://aclweb.org/anthology/C16-2015).\n",
    "\n",
    "### First we build a document frequency model based on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "#NLTK dependencies that must be installed first\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "from pke import compute_document_frequency\n",
    "from string import punctuation\n",
    "\n",
    "# path to the collection of documents\n",
    "input_dir = '/z/aolney/repos/jedm-reviewer-finder/texts/'\n",
    "\n",
    "# path to the DF counts dictionary, saved as a gzip tab separated values\n",
    "output_file = 'edm-df.gz'\n",
    "\n",
    "# compute df counts and store stem -> weight values\n",
    "compute_document_frequency(input_dir=input_dir,\n",
    "                           output_file=output_file,\n",
    "                           format=\"raw\",            # input files format\n",
    "                           use_lemmas=False,    # do not use Stanford lemmas\n",
    "                           stemmer=\"porter\",            # use porter stemmer\n",
    "                           stoplist=list(punctuation),            # stoplist\n",
    "                           delimiter='\\t',            # tab separated output\n",
    "                           extension='txt',          # input files extension\n",
    "                           n=5)              # compute n-grams up to 5-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "### Use KPMiner with our custom DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "import pke\n",
    "\n",
    "output = []\n",
    "\n",
    "#load df\n",
    "df = pke.load_document_frequency_file(input_file='/z/aolney/repos/jedm-reviewer-finder/edm-df.gz')\n",
    "\n",
    "\n",
    "#must loop over each document in texts\n",
    "with open('/z/aolney/repos/jedm-reviewer-finder/texts.tsv') as inputFile:\n",
    "    for line in inputFile:\n",
    "        split = line.split(\"\\t\")\n",
    "        text = split[1]\n",
    "        hashCode = split[0]\n",
    "        \n",
    "        # 1. create a KPMiner extractor.\n",
    "        extractor = pke.unsupervised.KPMiner(language='english')\n",
    "\n",
    "        # 2. load the content of the document.\n",
    "        #extractor.read_document(format='raw')\n",
    "        extractor.read_text(text)\n",
    "\n",
    "        # 3. select {1-5}-grams that do not contain punctuation marks or\n",
    "        #    stopwords as keyphrase candidates. Set the least allowable seen\n",
    "        #    frequency to 5 and the number of words after which candidates are\n",
    "        #    filtered out to 200.\n",
    "        lasf = 3 #5\n",
    "        cutoff = 400 #200\n",
    "        stoplist = nltk.corpus.stopwords.words(\"english\")\n",
    "        extractor.candidate_selection(lasf=lasf, cutoff=cutoff, stoplist=stoplist)\n",
    "\n",
    "        # 4. weight the candidates using KPMiner weighting function.\n",
    "        alpha = 2.3\n",
    "        sigma = 3.0\n",
    "        extractor.candidate_weighting(df=df, alpha=alpha, sigma=sigma)\n",
    "\n",
    "        # 5. get the 10-highest scored candidates as keyphrases\n",
    "        keyphrases = extractor.get_n_best(n=10)\n",
    "        \n",
    "        #NOTE: we convert score float to int for compression purposes\n",
    "        for phrase,score in keyphrases:\n",
    "            output.append( hashCode + \"\\t\" + phrase + \"\\t\" + str(int(score)) + \"\\n\" )\n",
    "        \n",
    "#write out\n",
    "with open(\"keys.tsv\", \"w\") as f:\n",
    "    f.writelines(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "ifsharp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A"
    ],
    [
     "ifsharp",
     "ifsharp",
     "",
     ""
    ]
   ],
   "version": "0.9.15.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
